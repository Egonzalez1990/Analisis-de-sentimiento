---
title: "Obligatorio Big Data en Inversiones"
author: "Camblor - González"
date: "5/5/2020"
output:
  pdf_document:
    df_print: kable
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Librerías
library(rtweet)
library(readr)
library(tidytext)
library(dplyr)
library(tidyr)
library(igraph)
library(ggraph)
library(tidyverse)
library(twitteR)
library(syuzhet)
library(quantmod)
library(TTR)
library(zoo)
library(randomForest)
library(lubridate)
library(widyr)

# Empresa elegida para el Análisis. TESLA
# 1- Descargamos la Información financiera de TESLA para los últimos 15 días
TSLA = getSymbols("TSLA", auto.assign = FALSE , from = '2020-04-24', to = '2020-05-05')

# 2- Descargamos los Twitters relacionados con Tesla para el Período de Estudio del Indice.

# Con esto nos permite Descargar Tweets Solo Una semana para atrás
# TESLA_tweets <- search_tweets(q = "tesla market", n = 5000,
#                               lang = "en",
#                               include_rts = FALSE,
#                               until = "2020-05-03",
#                               #geocode = lookup_coords("usa"),
#                               retryonratelimit = TRUE,
#                               type = "mixed")
#
# Musk_tweets <- search_tweets(q = "Elon Musk", n = 1000,
#                               lang = "en",
#                               include_rts = FALSE,
#                               until = "2020-05-03",
#                               #geocode = lookup_coords("usa"),
#                               retryonratelimit = FALSE,
#                               type = "mixed")
#
# save(TESLA_tweets,file="C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/TESLA_tweets.RData")
#
# save(Musk_tweets,file="C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/Musk_tweets.RData")
# 
# TESLA_tweetsNEW <- search_tweets(q = "tesla market", n = 4000,
#                               lang = "en",
#                               include_rts = FALSE,
#                               since = "2020-05-03",
#                               until = "2020-05-05",
#                               #geocode = lookup_coords("usa"),
#                               retryonratelimit = TRUE,
#                               type = "mixed")
# 
# Musk_tweetsNEW <- search_tweets(q = "Elon Musk", n = 1000,
#                               lang = "en",
#                               include_rts = FALSE,
#                               since = "2020-05-03",
#                               until = "2020-05-05",
#                               #geocode = lookup_coords("usa"),
#                               retryonratelimit = FALSE,
#                               type = "mixed")

# save(TESLA_tweetsNEW,file="C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/TESLA_tweetsNEW.RData")
# 
# save(Musk_tweetsNEW,file="C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/Musk_tweetsNEW.RData")

load("C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/Musk_tweets.RData")
load("C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/TESLA_tweets.RData")
load("C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/TESLA_tweetsNEW.RData")
load("C:/Users/emili/Desktop/ORT Analitica/Big data en inversiones/Obligatorio/Musk_tweetsNEW.RData")
# Unir las Bases 
Bdtweet <- rbind(TESLA_tweets, Musk_tweets, TESLA_tweetsNEW, Musk_tweetsNEW)

#Selecciono las Columnas para el análisis
Bdtweet <- Bdtweet %>% select(2,3,5,13,14)

# Balanceo los tweets por dia
Bdtweet %>% mutate(diatw = format(created_at, "%m-%d")) %>% group_by(diatw) %>% summarise(n = n())
set.seed(100)
Bdtweet <- Bdtweet %>% mutate(diatw = format(created_at, "%m-%d")) %>% group_by(diatw) %>%  sample_n(100)

## Limpiea de Texto
Bdtweet$cleantext <- gsub("^[[:space:]]*","",Bdtweet$text) # Remove leading whitespaces
Bdtweet$cleantext <- gsub("[[:space:]]*$","",Bdtweet$cleantext) # Remove trailing whitespaces
Bdtweet$cleantext <- gsub(" +"," ",Bdtweet$cleantext) #Remove extra whitespaces
Bdtweet$cleantext <- gsub("'", "%%", Bdtweet$cleantext) #Replace apostrophes with %%
Bdtweet$cleantext <- iconv(Bdtweet$cleantext, "latin1", "ASCII", sub="") # Remove emojis
Bdtweet$cleantext <- gsub("<(.*)>", "", Bdtweet$cleantext) #Remove Unicodes like <U+A>
Bdtweet$cleantext <- gsub("\\ \\. ", " ", Bdtweet$cleantext) #Replace orphaned fullstops with space
Bdtweet$cleantext <- gsub("  ", " ", Bdtweet$cleantext) #Replace double space with single space
Bdtweet$cleantext <- gsub("%%", "\'", Bdtweet$cleantext) #Change %% back to apostrophes
Bdtweet$cleantext <- gsub("https(.*)*$", "", Bdtweet$cleantext) #Remove tweet URL
Bdtweet$cleantext <- gsub("\\n", "-", Bdtweet$cleantext) #Replace line breaks with "-"
Bdtweet$cleantext <- gsub("--", "-", Bdtweet$cleantext) #Remove double "-" from double line breaks
Bdtweet$cleantext <- gsub("&amp;", "&", Bdtweet$cleantext) #Fix ampersand &
Bdtweet$cleantext[Bdtweet$cleantext == " "] <- "<no text>"

# Tokenizar el texto Limpio

limpiar_tokenizar <- function(texto){
  # El orden de la limpieza no es arbitrario
  # Se convierte todo el texto a minúsculas
  nuevo_texto <- tolower(texto)
  # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
  # de cualquier cosa que no sea un espacio)
  nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
  # Eliminación de signos de puntuación
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
  # Eliminación de números
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
  # Eliminación de espacios en blanco múltiples
  nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
  # Tokenización por palabras individuales
  nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
  # Eliminación de tokens con una longitud < 2
  nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
  return(nuevo_texto)
}

# Se aplica la función de limpieza y tokenización a cada tweet
Bdtweet <- Bdtweet %>% mutate(texto_tokenizado = map(.x = cleantext,
                                                   .f = limpiar_tokenizar))

# Generar la base con columna de palabras por tweet
bd_tidy <- Bdtweet %>% select(-c(3,6,7)) %>% unnest()
bd_tidy <- bd_tidy %>% rename(token = texto_tokenizado)
head(bd_tidy) 

# Palabras más usadas por día
bd_tidy %>% group_by(diatw, token) %>% count(token) %>% group_by(diatw) %>% top_n(10, n) %>% arrange(diatw, desc(n)) %>% print(n=100)

# Se filtran las stopwords
bd_tidy_stop <- bd_tidy %>% filter(!(token %in% stop_words$word))
# Se repien pralabras más usadas por día
bd_tidy_stop %>% group_by(diatw, token) %>% count(token) %>% group_by(diatw) %>% top_n(10, n) %>% arrange(diatw, desc(n)) %>% print(n=100)

# Analisis de sentimiento
# Diccionario Bing. Clasificación binaria
sentimientos <- get_sentiments(lexicon = "bing")
head(sentimientos)

#Recodificción
sentimientos <- sentimientos %>%
  mutate(valor = if_else(sentiment == "negative", -1, 1))

#Aignación de senimiento a cada Token
tweets_sent <- inner_join(x = bd_tidy_stop, y = sentimientos,
                          by = c("token" = "word"))

#Se suman los sentimientos de las palabras que forman cada tweet.
#Porcentaje de tweets Negativos positivos y neutros por día

TablaPorcentaje <- tweets_sent %>% group_by(status_id, diatw) %>%
                    summarise(sentimiento_promedio = sum(valor)) %>%
                    group_by(diatw) %>%
                    summarise(positivos = 100 * sum(sentimiento_promedio > 0) / n(),
                              neutros = 100 * sum(sentimiento_promedio == 0) / n(),
                              negativos = 100 * sum(sentimiento_promedio  < 0) / n())



# Tweets ponderados
TweetsPonderados <- tweets_sent %>% ungroup() %>%  select(-3) %>% mutate(ValorPonderado = valor*(1+(0.01*(tweets_sent$favorite_count)+0.05*tweets_sent$retweet_count)))

#Se suman los sentimientos de las palabras que forman cada tweet.
#Porcentaje de tweets Negativos positivos y neutros por día

TweetsPonderados %>% group_by(status_id, diatw, sentiment) %>%
  summarise(sentimiento_promedio = sum(ValorPonderado)) %>%
  group_by(diatw, sentiment) %>%
  summarise(suma = sum(sentimiento_promedio)) %>% spread(key = sentiment, value = suma) %>% 
  mutate(Indice = positive/(positive+abs(negative))) %>%  
  ungroup() %>%
  ggplot(aes(x = diatw, y = Indice,  group = 1)) +
  geom_point() + 
  geom_line() + 
  labs(x = "fecha de publicación") +
  theme_bw() +
  theme(legend.position = "none")

BaseGrafIndice <- TweetsPonderados %>% group_by(status_id, diatw, sentiment) %>%
  summarise(sentimiento_promedio = sum(ValorPonderado)) %>%
  group_by(diatw, sentiment) %>%
  summarise(suma = sum(sentimiento_promedio)) %>% spread(key = sentiment, value = suma) %>% 
  mutate(Indice = positive/(positive+abs(negative))) 

```

## Introducción

Twitter es actualmente una dinámica y gran fuente de contenidos que, dada su popularidad e impacto, se ha convertido en la principal fuente de información para estudios de Social Media Analytics. Su utilización para el análisis de reputación de empresas, productos o personalidades, estudios de impacto relacionados con marketing, extracción de opiniones y predicción de tendencias son sólo algunos ejemplos de aplicaciones. 
En este contexto, decidimos realizar un análisis partiendo de Twitter tomando como foco tanto a la empresa Tesla como a su CEO Elon Musk. Musk presenta una grado de participación más que significativo en la red social, teniendo casi 4 millones de seguidores y mencionando activamente a la empresa.
Nos motivó este caso a raíz de un tweet realizado por Musk el día 1 de Mayo, donde afirmaba que "las acciones de Tesla estaban demasiado altas". Se sospecha que sus declaraciones podrían ser una de las causas de que las propias acciones de Tesla sufrieran una baja en la bolsa de valores del 9,3%.
Es de nuestro interés obtener información sobre la sensibilidad del público y su repercusión en la empresa ante la interacción en la mencionada red social.


```{r echo=FALSE, fig.width=6, fig.height=4}
#Gráfico evolución del Indice
plot(TSLA[,4] , main = "Evolucion del Indice")
```

## Obtención, limpieza y estandarización de datos

Obtuvimos los tweets generados con las palabras claves “tesla" y "market” utilizadas en un mismo tweet así como “Elon Musk”, para el período entre el 25 de abril y el 04 de Mayo de 2020, y, mediante la combinación de ambas bases llegamos a una única base de datos a partir de la cual realizar nuestro análisis.
Vale destacar que, dadas las restircciones de la api de twitter para la descarga de los mismos, se creo una base balanceada de 100 tweets diarios. 
Los datos obtenidos de twitter, así como de cualquier red social con la que trabajamos, deben atravesar un proceso de limpieza que permitan extraer información útil, con estructura y contenido. Trabajaremos tanto con números, fechas y textos. El manejo de datos tipo cadenas, son complejos y requiere mucho esfuerzo por lo que consideramos como quitar números y puntuación, evitar palabras como “y”, “pero” y “o”; quitar emojis y cómo separar las oraciones en palabras individuales (tokenization).
La tokenización nos permite dividir el texto en las unidades que lo conforman, entendiendo por unidad el elemento más sencillo con significado propio para el análisis en cuestión, en este caso, las palabras.
Tras realizar la limpieza y tokenización a cada tweet, se modificó la base generando una columna de palabras por tweet. Lo que permitió obtener de manera más sencilla cuáles eran las palabras más utilizadas por día.


```{r echo=FALSE, fig.width=7, fig.height=6.5}
#Representación gráfica de las frcuencias. 
bd_tidy_stop %>% group_by(diatw, token) %>% count(token) %>% group_by(diatw) %>%
  top_n(10, n) %>% arrange(diatw, desc(n)) %>%
  ggplot(aes(x = reorder(token,n), y = n, fill = diatw)) +
  geom_col() +
  theme_bw() +
  labs(y = "", x = "") +
  theme(legend.position = "none") +
  coord_flip() +
  facet_wrap(~diatw,scales = "free", ncol = 2, drop = TRUE)

```

## Análsis de Sentimientos


Comenzamos por asignar un sentimiento a cada token haciendo uso del diccionario de R “bing” que clasifica las palabras de forma binaria como positivas o negativas.
Para facilitar el cálculo de sentimientos se recodifican los sentimientos de cada Token como +1 para positivo y -1 para negativo.

Esto nos permite realizar el análisis de sentimiento de cada tweet considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman. Si bien no es la única forma abordar el análisis de sentimientos se consigue un buen equilibrio entre complejidad y resultados.

Como resultado, obtenemos los tweets clasificados como negativos, positivos y neutros por día.

```{r echo=FALSE}
#Representación gráfica de las frcuencias. 
TablaPorcentaje

#Graficamente

tweets_sent %>% group_by(status_id, diatw) %>%
  summarise(sentimiento_promedio = sum(valor)) %>%
  group_by(diatw) %>%
  summarise(positivos = 100 * sum(sentimiento_promedio > 0) / n(),
            neutros = 100 * sum(sentimiento_promedio == 0) / n(),
            negativos = 100 * sum(sentimiento_promedio  < 0) / n()) %>% 
  ungroup() %>%
  gather(key = "sentimiento", value = "valor", -diatw) %>%
  ggplot(aes(x = diatw, y = valor, fill = sentimiento)) + 
  geom_col(position = "dodge", color = "black")  +
  theme_bw()
```

Finalmente se ponderaron los token de los distintos tweets en base a aquellas métricas de comportamiento que permanecen públicas y pueden verse a simple vista al visitar cualquier perfil.
Retuits: número de ocasiones en las que se retuiteó una publicación.
Favoritos: número de ocasiones en las que se marcó como favorito una publicación.
Esto nos permite ponderar los token de cada tweets en función de la visibilidad de los mismos que surje de estas métricas.
La ponderación de los token se realiza asignandoles un incremento de un 1% por favorito obtenido por el tweet correspondiente y un incremento de un 4% por cada retweet obtenido.


```{r echo=FALSE}
head(TweetsPonderados)
```

## Indice de Sentimiento

Con los datos ponderados se construye un índice diario calculada a partir de la suma de los sentimentos positivos ponderados sobre los sentimientos totales ponderados por día.

```{r echo=FALSE}
BaseGrafIndice
```

## Conclusión
Habiendo realizado los procedimientos necesarios para la correcta extracción y preparación de los datos, podemos pasar a una etapa de observación y análisis de los mismos.
Aunque los resultados financieros de la empresa pueden estar sujetos a distintos factores, se ve una clara relación entre los sentimientos predominantes por día en twitter y la evolución del índice.

```{r echo=FALSE, fig.width=6, fig.height=4}
TweetsPonderados %>% group_by(status_id, diatw, sentiment) %>%
  summarise(sentimiento_promedio = sum(ValorPonderado)) %>%
  group_by(diatw, sentiment) %>%
  summarise(suma = sum(sentimiento_promedio)) %>% spread(key = sentiment, value = suma) %>% 
  mutate(Indice = positive/(positive+abs(negative))) %>%  
  ungroup() %>%
  ggplot(aes(x = diatw, y = Indice,  group = 1)) +
  geom_point() + 
  geom_line() + 
  labs(x = "fecha de publicación") +
  theme_bw() +
  theme(legend.position = "none")

plot(TSLA[,4] , main = "Evolucion del Indice")
```

El día 1 de mayo se puede observar con mucha claridad cómo hay un pico decreciente tanto la bolsa de valores como en nuestro índice de sentimiento (alta valoración de sentimientos negativos). Esto puede explicarse por la hipotesis del tweet mencionado en el primer párrafo.
En conclusión para este caso se observa una correlación interesante entre el análsiis de sentimiento y el desempeño financiero de la empresa estudiada.  


